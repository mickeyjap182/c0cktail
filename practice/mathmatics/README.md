# 数学メモ
仕掛中

DeepLerningの基本数学

[数学1](https://lib-arts.hatenablog.com/entry/math_nn4)
[数学2](https://lib-arts.hatenablog.com/entry/math_nn5)
[数学3](https://lib-arts.hatenablog.com/entry/math_nn6)

## Overview
- 機械学習や深層学習の分野を理解するための基礎的な数学を理解する。（関数、微積、数列、極限、ベクトル、行列、確率、集合）
## 関数
f(x)で表す式。ニューラルネットワークも関数で表現する。
## 微分積分
-  __微分__ 機械学習のアルゴリズムはいくつかの勾配（変数が多い場合の微分の値）を基にする。__「微小区間における傾きの極限」__ を表す。
-  __積分__
### 微分
- f(x)に対する微分値は、f'(x)で表す。
- 定点における傾きを表す。
#### 最小値問題
f(x)の最小値を求める。2通りの解法がある。
- 微分式f'(x)が0となる値で求める。二次以上の方程式では、傾きが0となる箇所の洗い出し以外に、凹凸方向にも注意(平方完成でも求められる)
- 漸化式で等比数列の極限値(無限大)にした時に収束する値で近似解を求める。厳密に解けない大多数はこちらの方法で解く。
`e.g.) f(x)=x2 、f′(x)=2x、α=0.25、x0=2のとき、xn+1=xn−αf′(xn)を求める` こちらは勾配降下法の式でもある。
#### 勾配下降法
係数を試行することで、最適な等比数列でデータ分布などを導き出す。
### 行列演算
- 和・差...同じ位置に対応する行列の成分同士を演算する。
- スカラー倍...全ての成分にスカラーをかける。どんな形の行列も演算できる。
- 積...行列A、Bの積を行う場合、`Aの列数とBの行数が等しい必要がある。`そして`Aの行数とBの列数の結果行列となる。`計算方法は`Aは行の左から、Bの列の上から開始し、スライドしながら各成分の同士の積を求めて足す。`
#### MLP推論
ニューラルネットワーク推論の計算のベースとなる行列の積算を使う。
- n個の入力層xが、m個の出力層yにかかり、xからyに向かう各`重み`wが、xに向かう。wa1,wa2...wan,...を[[wa1,wa2...wam],[wb1,wb2...wbm],...] * [x1,x2...xn]の行列演算で求める。
- 回帰分析の単回帰(y=wx+b)の入力・出力を複数にした演算にも行列積算(y,xが行列)で対応できる。
- 計算式を入れ子にすることで多層の計算が可能。 `y' = w'σφ(wx+b) + b'`
φ:中間層の判断。非線形関数にすることで、中間層の表現力をあげられる。
- ここでの非線形関数を活性化関数と呼ぶ。(旧シグモイド関数を指したり、現在は[ReLU系](https://qiita.com/namitop/items/d3d5091c7d0ab669195f)が主流)
### 回帰分析
散布図から分布を予測する式（直線式=一次関数）`y=ax+b`を求める。
#### 1. モデル数式の定義
モデル数式の定義(学習)。xの値を元にyの値を予測する。a, bは仮の実数値で検証。
#### 2. 誤差関数の定義
パラメータa,bを決定する誤差関数を定義する。n個のサンプルに対して、(予測値y' - 実測値y )の二乗誤差(分散)の値を近づける。
- __予測値 y' = ax + b__ ...①(単回帰（一次関数式）に収束するとして)
- __予測値(k) = Σ(K=1->n)(y' - y) ^2__ ...②(K1 - Knまでの各サンプルにおける二乗誤差の和)
- __誤差関数 E(θ) = E(a,b) = Σ(K=1->n)(ax(k) + b -y(k)) ^2__ ...③(②に①を代入する)
- 最小二乗法...二乗誤差関数として定義する。予測値とサンプルの実測値との佐野に需要
- 微分(きれいにはまる場合)や勾配降下法(近似値を求める場合)を使う。
#### 3. 最適化
誤差関数 E(a,b)の最小値を求める(最適化)
- 微分(y=0)で解く
誤差関数E(a,b)の偏微分（各x, yの値を定数に置き換えた2つの連立方程式）で解く。
- 勾配効果法で解く
a, bの繰り返し演算により最適な値を求める。複雑なものは微分では解けないので、近似的に最適解を求めるにはこちらを使う。

### 積分
